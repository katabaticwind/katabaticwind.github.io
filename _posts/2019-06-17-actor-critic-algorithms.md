---
layout: post
title:  "Reinfocement Learning: Actor-Critic Algorithms"
date: 2019-06-22
---

## Introduction
- Policy gradient methods work pretty well, but they are quite slow because it requires *a lot* of experience to come up with decent Monte Carlo samples.
- Actor-Critic methods take the policy gradient update and convert it into a "bootstrap" update. This alteration can speed up training, but might be a bit less stable.
- Recall that the vanilla policy gradient algorithm attempts to directly improve a random policy by moving in the direction of highest expected return,
$$\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)$$, where $$J(\theta)$$ is the expected return under policy $$\pi_{\theta}$$.
- This gradient term depends on the policy network probabilities, $$\pi_{\theta}(a_t \vert s_t)$$, and the rewards generated in each episode, $$R_t$$: $$J(\theta) \approx \sum_{t=1}^T \log \pi_{\theta}(a_t \vert s_t) R_t$$.
- Replacing the expected return by a single sample results in a *noisy* estimate, which flows into policy gradient updates (and you will therefore hear people say that vanilla policy gradient has high variance).
- You can reduce the variance of the updates by subtracting a "baseline" from $$R_t$$. A baseline that is itself an estimate of the expected reward can result in a much smaller weights, $$R_t - b_t \ll R_t$$. In other words, subtracting a baseline results in a *re-scaling* of the updates.

Actor-critic methods replace both terms of the above expression with approximations based on the value function. First, we want $$b_t$$ to be an approximation of the value function, so we'll replace it by the output of a network  $$\hat{V}^{\pi}_{\phi}(s_t) \approx V^{\pi}(s_t)$$. Second, according to the Bellman equation, the expected reward-to-go is equal to the expected one-period reward plus the following expected return: $$E[R_t] = E \left[r(s_t, a_t) + V^{\pi}(s_t') \right]$$. If we knew the value function, then we could estimate this quantity by evaluating under random sample transitions $$\{s_t, a_t, r_t, s_t'\}$$. Instead, we add a second approximation error by replacing the true value function with the sample network that we used above. So our full estimate of the second term becomes $$E[R_t] \approx r(s_t, a_t) + \hat{V}_{\phi}^{\pi}(s_t')$$. We call this a "bootstrap" estimator because we are estimating $$V$$ based in part on values generated by our estimate.

For comparison, we could also estimate $$E[R_t]$$ using the observed reward-to-go, $$\sum_{\tau = t}^{T} r(s_{\tau}, a_{\tau})$$. This is a Monte-Carlo estimator.

## Details

**Algorithm (Batch Actor-Critic)**
1. Sample experiences $$\{s_i, a_i, r_i, s'_i\}_{i=1}^N$$ using policy $$\pi_{\theta}(a_i \vert s_i)$$
2. Calculate value targets $$\{y_i\}_{i=1}^N$$ defined as $$y_i = r_i + \gamma V_{\phi}^{\pi}(s_i')$$
3. Update $$V_{\phi}^{\pi}$$ by minimizing $$\|y_i - V_{\phi}^{\pi}(s_i')\|^2_2$$
4. Calculate policy targets $$\{A_i\}_{i=1}^N$$ defined as $$A_i = y_i - \gamma V_{\phi}^{\pi}(s_i')$$
5. Update $$\pi_{\theta}(a_i \vert s_i)$$ by maximizing $$\sum_{i=1}^N \log \pi_{\theta}(a_i \vert s_i) A_i$$

**Notes** 1. In step (3) the targets $$y_i$$ are treated as constants (even though they depend on the value network). 2. In step (4) we are using the updated value network to re-calculate $$y_i$$ and $$V_{\phi}^{\pi}(s_i)$$. 3. In step (5) the targets $$A_i$$ are treated as constants (or else $$\phi$$ will also update--we only want to update $$\theta$$ in this step).


## Tensorflow
- In this implementation I'll use two networks, one to represent the actor ($$\pi$$), and one to represent the critic ($$V$$). It's also possible to train a single network with two branches. This might make sense if the network contains convolutional layers so as to provide each branch with the same visual information.
- The actor-critic algorithm is similar to the basic policy gradient algorithm, so we can mostly use the same routines from before. The main "gotcha" is making sure that training operations modify the correct parameters based on the correct errors. In step (2), we calculate targets $$y_i$$ using the value network $$V_{\phi}^{\pi}$$. In the following step we want to update the parameters $$\phi$$ based on $$V_{\phi}^{\pi}(s_i')$$, treating the $$y_i$$ as fixed. The simple way to do this is to calculate the targets and the update in separate steps:
```
targets = sess.run(values, feed_dict={states_pl: states})
sess.run(values_update, feed_dict={states_pl: next_states, targets_pl: targets})
```
- Now once again in the policy update, $$A_i$$ involves the value network, so an update that directly used the $$A_i$$ *tensor* would update $$\theta$$ and $$\phi$$. We can do the same thing as before, pre-calculating the weights $$A_i$$, and then feeding these as constants in the policy update operation. First, we get new $$y_i$$ targets based on the updated value network
```
targets = sess.run(values, feed_dict={states_pl: states}),
```
then we calculate the weights and update the policy network
```
weights = sess.run(weights, feed_dict={states_pl: next_states})
sess.run(policy_update, feed_dict={states_pl: states, actions_pl: actions, weights_pl: weights})
```
- The rest of the implementation basically follows the policy gradient implementation.  
