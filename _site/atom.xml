<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>katabaticwind's blog</title>
  <id>http://localhost:4000</id>
  <link href="http://localhost:4000" />
  <link href="http://localhost:4000/atom.xml" rel="self" />
  <updated>2019-06-26T10:26:09-04:00</updated>
  <rights>Copyright 2014, Anonymous</rights>
  <author>
    <name>Anonymous</name>
  </author>
  
    <entry>
      <title>Reinfocement Learning: Actor-Critic Algorithms</title>
      <link href="http://localhost:4000/2019/06/22/actor-critic-algorithms/" />
      <summary type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Policy gradient methods work pretty well, but they are quite slow because it requires &lt;em&gt;a lot&lt;/em&gt; of experience to come up with decent Monte Carlo samples.&lt;/li&gt;
  &lt;li&gt;Actor-Critic methods take the policy gradient update and convert it into a “bootstrap” update. This alteration can speed up training, but might be a bit less stable.&lt;/li&gt;
  &lt;li&gt;Recall that the vanilla policy gradient algorithm attempts to directly improve a random policy by moving in the direction of highest expected return,
&lt;script type=&quot;math/tex&quot;&gt;\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;J(\theta)&lt;/script&gt; is the expected return under policy &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;This gradient term depends on the policy network probabilities, &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_t \vert s_t)&lt;/script&gt;, and the rewards generated in each episode, &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;J(\theta) \approx \sum_{t=1}^T \log \pi_{\theta}(a_t \vert s_t) R_t&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Replacing the expected return by a single sample results in a &lt;em&gt;noisy&lt;/em&gt; estimate, which flows into policy gradient updates (and you will therefore hear people say that vanilla policy gradient has high variance).&lt;/li&gt;
  &lt;li&gt;You can reduce the variance of the updates by subtracting a “baseline” from &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt;. A baseline that is itself an estimate of the expected reward can result in a much smaller weights, &lt;script type=&quot;math/tex&quot;&gt;R_t - b_t \ll R_t&lt;/script&gt;. In other words, subtracting a baseline results in a &lt;em&gt;re-scaling&lt;/em&gt; of the updates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Actor-critic methods replace both terms of the above expression with approximations based on the value function. First, we want &lt;script type=&quot;math/tex&quot;&gt;b_t&lt;/script&gt; to be an approximation of the value function, so we’ll replace it by the output of a network  &lt;script type=&quot;math/tex&quot;&gt;\hat{V}^{\pi}_{\phi}(s_t) \approx V^{\pi}(s_t)&lt;/script&gt;. Second, according to the Bellman equation, the expected reward-to-go is equal to the expected one-period reward plus the following expected return: &lt;script type=&quot;math/tex&quot;&gt;E[R_t] = E \left[r(s_t, a_t) + V^{\pi}(s_t') \right]&lt;/script&gt;. If we knew the value function, then we could estimate this quantity by evaluating under random sample transitions &lt;script type=&quot;math/tex&quot;&gt;\{s_t, a_t, r_t, s_t'\}&lt;/script&gt;. Instead, we add a second approximation error by replacing the true value function with the sample network that we used above. So our full estimate of the second term becomes &lt;script type=&quot;math/tex&quot;&gt;E[R_t] \approx r(s_t, a_t) + \hat{V}_{\phi}^{\pi}(s_t')&lt;/script&gt;. We call this a “bootstrap” estimator because we are estimating &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; based in part on values generated by our estimate.&lt;/p&gt;

&lt;p&gt;For comparison, we could also estimate &lt;script type=&quot;math/tex&quot;&gt;E[R_t]&lt;/script&gt; using the observed reward-to-go, &lt;script type=&quot;math/tex&quot;&gt;\sum_{\tau = t}^{T} r(s_{\tau}, a_{\tau})&lt;/script&gt;. This is a Monte-Carlo estimator.&lt;/p&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Algorithm (Batch Actor-Critic)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample experiences &lt;script type=&quot;math/tex&quot;&gt;\{s_i, a_i, r_i, s'_i\}_{i=1}^N&lt;/script&gt; using policy &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_i \vert s_i)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Calculate value targets &lt;script type=&quot;math/tex&quot;&gt;\{y_i\}_{i=1}^N&lt;/script&gt; defined as &lt;script type=&quot;math/tex&quot;&gt;y_i = r_i + \gamma V_{\phi}^{\pi}(s_i')&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}&lt;/script&gt; by minimizing &lt;script type=&quot;math/tex&quot;&gt;\|y_i - V_{\phi}^{\pi}(s_i')\|^2_2&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Calculate policy targets &lt;script type=&quot;math/tex&quot;&gt;\{A_i\}_{i=1}^N&lt;/script&gt; defined as &lt;script type=&quot;math/tex&quot;&gt;A_i = y_i - \gamma V_{\phi}^{\pi}(s_i')&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_i \vert s_i)&lt;/script&gt; by maximizing &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N \log \pi_{\theta}(a_i \vert s_i) A_i&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt; 1. In step (3) the targets &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; are treated as constants (even though they depend on the value network). 2. In step (4) we are using the updated value network to re-calculate &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}(s_i)&lt;/script&gt;. 3. In step (5) the targets &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; are treated as constants (or else &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; will also update–we only want to update &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in this step).&lt;/p&gt;

&lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In this implementation I’ll use two networks, one to represent the actor (&lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;), and one to represent the critic (&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;). It’s also possible to train a single network with two branches. This might make sense if the network contains convolutional layers so as to provide each branch with the same visual information.&lt;/li&gt;
  &lt;li&gt;The actor-critic algorithm is similar to the basic policy gradient algorithm, so we can mostly use the same routines from before. The main “gotcha” is making sure that training operations modify the correct parameters based on the correct errors. In step (2), we calculate targets &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; using the value network &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}&lt;/script&gt;. In the following step we want to update the parameters &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; based on &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}(s_i')&lt;/script&gt;, treating the &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; as fixed. The simple way to do this is to calculate the targets and the update in separate steps:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;targets = sess.run(values, feed_dict={states_pl: states})
sess.run(values_update, feed_dict={states_pl: next_states, targets_pl: targets})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now once again in the policy update, &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; involves the value network, so an update that directly used the &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; &lt;em&gt;tensor&lt;/em&gt; would update &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. We can do the same thing as before, pre-calculating the weights &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt;, and then feeding these as constants in the policy update operation. First, we get new &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; targets based on the updated value network
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;targets = sess.run(values, feed_dict={states_pl: states}),
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;then we calculate the weights and update the policy network&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;weights = sess.run(weights, feed_dict={states_pl: next_states})
sess.run(policy_update, feed_dict={states_pl: states, actions_pl: actions, weights_pl: weights})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;The rest of the implementation basically follows the policy gradient implementation.&lt;/li&gt;
&lt;/ul&gt;
</summary>
      <id>http://localhost:4000/2019/06/22/actor-critic-algorithms/</id>
      <updated>2019-06-22T00:00:00-04:00</updated>
      <author>
        <name>Anonymous</name>
      </author>
    </entry>
  
    <entry>
      <title>ICML 2019: Invited Talks</title>
      <link href="http://localhost:4000/2019/06/17/icml-2019-invited-talks/" />
      <summary type="html">&lt;p&gt;Last week I attended the International Conference on Machine Learning (ICML) in Long Beach, CA. ICML is one of the most important conferences on machine learning in the world, attracting paper submissions and hosting invited talks by top researchers in the field, such as Yann LeCun and David Silver, as well as small armies of researchers from industry outfits such as DeepMind, OpenAI, and Google Brain. In the coming weeks I’ll be writing about the papers I thought were most interesting and inspiring, but for now I thought I might give a little summary of the invited talks. These talks aren’t really machine learning talks per se, but they encourage listeners to think about how machine learning is used in the real world, and perhaps to reflect and consider a bigger picture for a few minutes.&lt;/p&gt;

&lt;h2 id=&quot;tuesday-june-11-john-abowd&quot;&gt;Tuesday, June 11: John Abowd&lt;/h2&gt;
&lt;p&gt;This was my first time attending ICML, and I didn’t know exactly what to expect. There was an optional tutorial session on Monday, but I decided to skip this and attend the workshops at the end of the week instead. The conference officially kicked off on Tuesday with an invited talk by John Abowd. John is an economist (possibly the only other economist by training at the conference other than myself) who is currently the U.S. Census Bureau’s associate director of research and methodology. I went to the talk with the expectation that it would be boring because, well, it’s the Census Bureau. But in fact the bureau faces a challenging problem, which is figuring out how to provide data (statistics) to users in useful while maintaining the privacy of its respondents. It turns out that there is a &lt;em&gt;fundamental&lt;/em&gt; trade-off between information and privacy: if you publish too many statistics too accurately, then no one has any privacy (and, conversely, perfect privacy implies &lt;em&gt;worthless&lt;/em&gt; data). John calls the “Fundamental Law of Information Recovery”.&lt;/p&gt;

&lt;p&gt;Essentially, the more statistics the bureau provides, the easier it is to match respondents against commercially available datasets (a “&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=3295691&quot;&gt;reconstruction attack&lt;/a&gt;”). To alleviate this problem, the Census Bureau adds randomness to the statistics it provides (or to the data used to generate statistics), which obviously reduces the accuracy of the statistics. This process is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;&gt;differential privacy&lt;/a&gt;, and John and his team have done a lot of great work putting this concept into practice at the Census Bureau, but at the end of the day someone will still need to decide how to balance privacy against accuracy–a task for which there is no guideline or mandate. None of this really means much to me, but it seems like an interesting line of research, and it is certainly important to Facebook, Google, and generally any group collecting large amounts of data about private data. If you’re interested, this &lt;a href=&quot;https://arxiv.org/abs/1809.02201&quot;&gt;paper&lt;/a&gt; describes some of the challenges John and his colleagues have encountered incorporating differential privacy at the Census Bureau. (If you’re &lt;em&gt;really&lt;/em&gt; interested, the complete 1940 census records have been available since 2012 if you want to play around with this stuff—you can find the data &lt;a href=&quot;https://1940census.archives.gov/index.asp&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;wednesday-june-12-aude-billard&quot;&gt;Wednesday, June 12: Aude Billard&lt;/h2&gt;
&lt;p&gt;The following day featured an invited talk by Aude Billard entitled “Machine Learning for Robots to Think Fast”. Dr. Billard is the head of the &lt;a href=&quot;http://lasa.epfl.ch&quot;&gt;LASA laboratory&lt;/a&gt; at the École Polytechnique Fédérale de Lausanne in Switzerland. She is one of the world’s leading experts in robotics. Her talk demonstrated a series of tasks that her lab had “trained” robotic arms to perform. In one task, a single arm—which includes a dexterous hand—tries to &lt;a href=&quot;https://www.youtube.com/watch?v=JEob5Slkvjw&quot;&gt;catch objects&lt;/a&gt; thrown in its direction. The researchers started by throwing objects that are simple to catch, like a small ball, and then moved on to more difficult items such as a tennis racquet or a half-full (or was it half-empty?) bottle of water. With the simple objects, the flight path is easier to predict, but the robot still needs to make a quick calculation to determine how to move its hand to meet the object. Catching a half-full bottle of water is more challenging because its path is irregular, and the robot may need to make a last-minute adjustment to its hand position to catch the bottle.&lt;/p&gt;

&lt;p&gt;There seem to be two keys to the robots success in this task. First, the researchers use direct demonstrations to teach the robot, that is, they throw the object and manually move the robot’s arm into position so that it experiences an approximately correct motion. Second, they combine physical modeling (dynamical systems) with machine learning techniques (e.g., Gaussian mixture models, support vector machines, and nonlinear/extended Kalman filters) to learn a model of the world. It’s a rather elegant—and clearly effective—combination of machine learning and applied mathematics, and a superb example of how machine learning compliments traditional science, especially in practical applications. (If this is sounds interesting, slides and code from a tutorial her lab recently led can be found &lt;a href=&quot;https://epfl-lasa.github.io/TutorialICRA2019.io/&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In another task, a pair of robotic arms learn to &lt;a href=&quot;https://www.youtube.com/watch?v=xIK6U52TjRM&quot;&gt;peel vegetables&lt;/a&gt;, which requires coordination between the arms as well as adaptation to the changes to the vegetable as it is peeled. At the end of the talk, Dr. Billard was asked what the biggest challenge in robotics is, and her answer was “general object manipulation”, and in particular the ability to handle objects and perform tasks that change the object’s shape.&lt;/p&gt;

&lt;h2 id=&quot;thursday-june-13-alison-gopnik&quot;&gt;Thursday, June 13: Alison Gopnik&lt;/h2&gt;
&lt;p&gt;Thursday morning featured an invited talk by cognitive psychologist and child cognition expert Alison Gopnik. Measured by audience engagement, this was the most popular invited talk of the conference, which might be because it was the least technical, and therefore best understood. Dr. Gopnik talked about how children learn and think in comparison with the adult mind, and suggested that researchers in the field of machine learning would do well to focus more on replicating the prior than the latter. I don’t think we want to make machines that function like children—children aren’t generally too reliable—but I can see the desirability of machines that &lt;em&gt;learn&lt;/em&gt; like children.&lt;/p&gt;

&lt;p&gt;Learning in childhood is characterized by three processes. First, children learn by building abstract causal models from data. They perform this task sub-consciously apparently, as we don’t believe any four-year olds explicitly understand graphical models, but their responses during carefully designed experiments are often consistent with Bayesian inference. (I don’t think this fact is too surprising or even unique to the child mind, but it’s at least a “promising feature” if we’re thinking of children as a model for learning). Second, children engage in an active learning process. You &lt;em&gt;can&lt;/em&gt; show children how to do things, but a considerable amount of what children learn to do is the result of trial and error. Children tend to explore more than adults. In an interesting experiment, children and adults select from a few boxes and receive rewards that depend on the box chosen. One box might be high-reward, high-risk, while another box might be low-risk, low-reward. Essentially, the subjects solve a one-armed bandit problem. What the researchers found is that children are more likely to test unlikely—but true—conclusions. In other words, children are much more likely to keep selecting boxes that have provided small rewards, which are the boxes (in one setup) that periodically deliver really big rewards. In contrast, after one or two small rewards adults will decide that such boxes should be avoided. As a result, the &lt;em&gt;long run&lt;/em&gt; strategies of children often outperform those of the adults.&lt;/p&gt;

&lt;p&gt;This idea isn’t new to researchers of reinforcement learning, where the the exploration-exploitation trade-off is understood to be crucially important. Children appear to be something like algorithms in their early stage: they do a lot of random shit, some of which ends up working. Adults act like algorithms in their later stages, rarely performing random actions, but instead exploiting their knowledge (or often just being stuck in a sub-optimal routine). And it seems that this stodgy learning style sets in fairly early in life, certainly by the time kids enter high school. It makes me think about my experience trying to teach college students, who (in my experience) always want to be told precisely how to do things, and never want to experiment. (It’s amazing how much pushback you will get for encouraging a students to try to figure things out on their own, &lt;em&gt;especially&lt;/em&gt; from faculty). On the other hand, experimentation is the nature of the scientist: keep trying new ideas and approaches. Perhaps a goal that we could all aspire to is to make time to pursue ideas &lt;strong&gt;with no expectation of reward&lt;/strong&gt;. Just do something because it seems interesting and see what happens. Probably nothing, but its not a waste of time. In fact, this should probably be thought of as one of the key roles of universities: to provide a space for researchers to &lt;em&gt;fail&lt;/em&gt;.&lt;/p&gt;
</summary>
      <id>http://localhost:4000/2019/06/17/icml-2019-invited-talks/</id>
      <updated>2019-06-17T11:03:00-04:00</updated>
      <author>
        <name>Anonymous</name>
      </author>
    </entry>
  
</feed>
