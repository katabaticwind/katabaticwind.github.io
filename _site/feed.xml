<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-07-08T10:01:14-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">katabaticwind</title><subtitle>A research and development blog.</subtitle><entry><title type="html">Recurrent Neural Networks</title><link href="http://localhost:4000/research/2019/07/05/recurrent-networks.html" rel="alternate" type="text/html" title="Recurrent Neural Networks" /><published>2019-07-05T00:00:00-04:00</published><updated>2019-07-05T00:00:00-04:00</updated><id>http://localhost:4000/research/2019/07/05/recurrent-networks</id><content type="html" xml:base="http://localhost:4000/research/2019/07/05/recurrent-networks.html">&lt;p&gt;An introduction to recurrent neural networks.&lt;/p&gt;</content><author><name>Colin Swaney</name></author><category term="research" /><category term="deep-learning" /><summary type="html">An introduction to recurrent neural networks.</summary></entry><entry><title type="html">Actor-Critic Methods</title><link href="http://localhost:4000/research/2019/07/03/actor-critic-methods.html" rel="alternate" type="text/html" title="Actor-Critic Methods" /><published>2019-07-03T00:00:00-04:00</published><updated>2019-07-03T00:00:00-04:00</updated><id>http://localhost:4000/research/2019/07/03/actor-critic-methods</id><content type="html" xml:base="http://localhost:4000/research/2019/07/03/actor-critic-methods.html">&lt;p&gt;There are two standard algorithms in reinforcement learning: Q-learning and the policy gradient method. These methods represent orthogonal approaches. Policy gradient methods work on the policy function; Q-learning works on the action-value function. Policy gradient methods use Monte Carlo approximation; Q-learning uses bootstrap updates. And finally, policy gradient methods are low bias, high variance, while Q-learning is a low variance, high bias approach. In statistical learning, there is always some balance of bias and variance that provides the best outcome. Actor-critic methods provide, in some sense, a compromise between Q-learning and policy gradient methods, and can be thought of as a way to balance out the costs and benefits of these approaches.&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;Recall that the vanilla policy gradient algorithm attempts to directly improve a random policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; by moving in the direction of highest expected return,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta \leftarrow \theta + \alpha \nabla_{\theta}J(\theta),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;J(\theta)&lt;/script&gt; is the expected return under policy &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}&lt;/script&gt;. The gradient term depends on the policy network probabilities, &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_t \vert s_t)&lt;/script&gt;, and the rewards generated in each episode, &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) \approx \sum_{t=1}^T \log \pi_{\theta}(a_t \vert s_t) R_t.&lt;/script&gt;

&lt;p&gt;The expression above is a very &lt;em&gt;noisy&lt;/em&gt; estimate of the expected return (because episode returns have a lot of variance), which flows into policy gradient updates. You will therefore hear people say that vanilla policy gradient is a “high variance” method. The standard approach to reduce the variance of the updates is to  subtract a “baseline” from &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt;. A baseline that is itself an estimate of the expected reward can result in a much smaller weights, &lt;script type=&quot;math/tex&quot;&gt;\|R_t - b_t \|   \ll \|R_t\|&lt;/script&gt;. In other words, subtracting a baseline results in a &lt;em&gt;re-scaling&lt;/em&gt; of the updates.&lt;/p&gt;

&lt;p&gt;Actor-critic methods replace both &lt;script type=&quot;math/tex&quot;&gt;R_t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b_t&lt;/script&gt; with approximations based on the value function. First, we want &lt;script type=&quot;math/tex&quot;&gt;b_t&lt;/script&gt; to be an approximation of the value function, so we replace it by the output of a network  &lt;script type=&quot;math/tex&quot;&gt;\hat{V}^{\pi}_{\phi}(s_t) \approx V^{\pi}(s_t)&lt;/script&gt;. Second, according to the Bellman equation, the expected reward-to-go is equal to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[R_t] = E \left[r(s_t, a_t) + V^{\pi}(s_t') \right].&lt;/script&gt;

&lt;p&gt;If we knew the value function, then we could estimate this quantity by evaluating it at random sample transitions &lt;script type=&quot;math/tex&quot;&gt;\{s_t, a_t, r_t, s_t'\}&lt;/script&gt;. Instead, we add a second approximation error by replacing the true value function with the sample network that we used above. So our estimate of the second term becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E[R_t] \approx r(s_t, a_t) + \hat{V}_{\phi}^{\pi}(s_t')&lt;/script&gt;

&lt;p&gt;The overall result is that we’ve replaced &lt;script type=&quot;math/tex&quot;&gt;R_t - b_t&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;r(s_t, a_t) + \hat{V}_{\phi}^{\pi}(s_t') - \hat{V}_{\phi}^{\pi}(s_t)&lt;/script&gt;. The latter expression can be seen as an approximation of the &lt;em&gt;advantage&lt;/em&gt; function, &lt;script type=&quot;math/tex&quot;&gt;A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t),&lt;/script&gt; and the resulting algorithm (as described here) is commonly referred to as the “advantage actor-critic” or “A2C” algorithm. We call this a “bootstrap” estimator because it estimates &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; based in part on previous estimate. For comparison, we could also approximate &lt;script type=&quot;math/tex&quot;&gt;E[R_t]&lt;/script&gt; using observed rewards,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{\tau = t}^{T} r(s_{\tau}, a_{\tau}),&lt;/script&gt;

&lt;p&gt;which gives an alternative, Monte-Carlo version of the A2C algorithm.&lt;/p&gt;

&lt;p&gt;The A2C algorithm performs two “sub-updates” per full update: one to update the value function, and one to update the policy. The first update is similar to the update performed in the DQN algorithm. There is a slight difference however. For one thing, we’re updated the value function instead of the action-value function. More importantly, the network used to find targets is the same network used to make predictions. In the DQN algorithm, we use a “cloned” network to find targets. The second update looks like a policy gradient update, and it is, except that we’ve approximated the reward and baseline. Therefore it seems reasonable to think of this algorithm as a combination of the two classics. That being said, why don’t we also use a “clone” network to perform the value update in the A2C algorithm? (I don’t know the answer, so this is really a question I’d like to know the answer to!)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;algorithm-batch-a2c&quot;&gt;Algorithm (Batch A2C)&lt;/h4&gt;
  &lt;ol&gt;
    &lt;li&gt;Sample experiences &lt;script type=&quot;math/tex&quot;&gt;\{s_i, a_i, r_i, s'_i\}_{i=1}^N&lt;/script&gt; using policy &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_i \vert s_i)&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Calculate value targets &lt;script type=&quot;math/tex&quot;&gt;\{y_i\}_{i=1}^N&lt;/script&gt; defined as &lt;script type=&quot;math/tex&quot;&gt;y_i = r_i + \gamma V_{\phi}^{\pi}(s_i')&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}&lt;/script&gt; by minimizing &lt;script type=&quot;math/tex&quot;&gt;\|y_i - V_{\phi}^{\pi}(s_i')\|^2_2&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Calculate policy targets &lt;script type=&quot;math/tex&quot;&gt;\{A_i\}_{i=1}^N&lt;/script&gt; defined as &lt;script type=&quot;math/tex&quot;&gt;A_i = y_i - \gamma V_{\phi}^{\pi}(s_i')&lt;/script&gt;&lt;/li&gt;
    &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;\pi_{\theta}(a_i \vert s_i)&lt;/script&gt; by maximizing &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N \log \pi_{\theta}(a_i \vert s_i) A_i&lt;/script&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt; 1. In step (3) the targets &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; are treated as constants (even though they depend on the value network). 2. In step (4) we are using the updated value network to re-calculate &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}(s_i)&lt;/script&gt;. 3. In step (5) the targets &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; are treated as constants (or else &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; will also update–we only want to update &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; in this step).&lt;/p&gt;

&lt;h3 id=&quot;aside-variance-and-bias&quot;&gt;Aside: Variance and Bias&lt;/h3&gt;
&lt;p&gt;I’d like to point out a few facts that help to compare the actor-critic method outlined above with the policy gradient method it mirrors. First, I want to say that the policy gradient algorithm is &lt;em&gt;unbiased&lt;/em&gt;, while the actor-critic method is (in general) &lt;em&gt;biased&lt;/em&gt;. What I mean by this statement is this: both algorithms work by approximating the expected return of policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;, and using that approximation to decide how to improve the policy. The value that we use to approximate the value we really want is called “unbiased” if it equals the true value on average; otherwise, its biased. The practical way to think about this is that unbiased methods will eventually bring you arbitrarily close to the truth if you collect enough data: if I played enough games, I would eventually get a very good idea of the expected return of &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;. A biased method method will &lt;em&gt;always&lt;/em&gt; leave some room for improve–even at infinity.&lt;/p&gt;

&lt;p&gt;Now for the second claim: the policy gradient approach has a higher variance than the actor-critic method. What do I mean by that? Essentially that the value I am using to approximate the true value I’m interested is expected to vary more under the policy gradient approach than under the actor-critic approach. The reason is that the policy gradient approach uses the full reward-to-go, which potentially varies &lt;em&gt;a lot&lt;/em&gt;, depending on the environment, whereas the actor-critic method uses a value that is typically close to zero (because they are regression residuals). The rest of the approximation is identical.&lt;/p&gt;

&lt;p&gt;Why does all this matter? &lt;em&gt;All&lt;/em&gt; statistical learning–to the extent that it attempts to learn the value of some function–is subject to a “bias-variance tradeoff”, whereby methods which lower bias tend to have higher variance, and vice-a-versa. What method works best in any particular application depends on what data is available, and, generally speaking, when there isn’t that much data available, low variance methods work better. In reinforcement learning, we’re really in a low-data environment when we consider the complexity of the system the agent is trying to learn and the limited number of times that it can interact with that environment. As an analogy, if our task was instead to estimate a regression model in a standard supervised learning task, if we are given only a small amount of data, then it may well be that a simple linear model outperforms nonlinear alternatives because those methods overfit small samples. The same thing is principle is at work here: we are accepting bias in our estimate in exchange of reduced variance, and the result tends to perform better, empirically (on interesting problems).&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In this implementation I’ll use two networks, one to represent the actor (&lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;), and one to represent the critic (&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;). It’s also possible to train a single network with two branches. This might make sense if the network contains convolutional layers so as to provide each branch with the same visual information.&lt;/li&gt;
  &lt;li&gt;The actor-critic algorithm is similar to the basic policy gradient algorithm, so we can mostly use the same routines from before. The main “gotcha” is making sure that training operations modify the correct parameters based on the correct errors. In step (2), we calculate targets &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; using the value network &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}&lt;/script&gt;. In the following step we want to update the parameters &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; based on &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}^{\pi}(s_i')&lt;/script&gt;, treating the &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; as fixed. The simple way to do this is to calculate the targets and the update in separate steps:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Now once again in the policy update, &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; involves the value network, so an update that directly used the &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt; &lt;em&gt;tensor&lt;/em&gt; would update &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. We can do the same thing as before, pre-calculating the weights &lt;script type=&quot;math/tex&quot;&gt;A_i&lt;/script&gt;, and then feeding these as constants in the policy update operation. First, we get new &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; targets based on the updated value network
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}),&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;then we calculate the weights and update the policy network&lt;/p&gt;
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy_update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights_pl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;The rest of the implementation basically follows the policy gradient implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;extensions&quot;&gt;Extensions&lt;/h2&gt;

&lt;h3 id=&quot;step-size&quot;&gt;Step Size&lt;/h3&gt;
&lt;p&gt;I mentioned an alternative Monte Carlo based approach earlier that simply uses the observed reward-to-go values, and I also mentioned that the actor-critic method is in some sense a combination of Q-learning and policy gradient with baseline. Now the question is, can I somehow vary the degree of the combination? Instead of approximating the reward-to-go by &lt;script type=&quot;math/tex&quot;&gt;r(s_t, a_t) + \gamma V_{\phi}^{\theta}(s_{t + 1})&lt;/script&gt;, I could expand the approximation one step and use&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r(s_t, a_t) + \gamma r(s_{t + 1}, a_{t + 1}) + \gamma^2 V_{\phi}^{\pi}(s_{t + 2})&lt;/script&gt;

&lt;p&gt;If I kept replacing &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; terms like this until I reached the end of the episode, then I’d end up with the full reward-to-go! In other words, as I increase the expansion my update gradually turns into the Monte Carlo algorithm. In terms of our implementation, the only difference is that we need to keep track of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;-step transitions, and feed these to the graph–the graph itself is exactly the same. Small values of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; (e.g., &lt;script type=&quot;math/tex&quot;&gt;N = 3, 4&lt;/script&gt;) can help stabilize the A2C learning curve in the CartPole environment the same way they help stabilize DQN.&lt;/p&gt;

&lt;h3 id=&quot;memory-replay&quot;&gt;Memory Replay&lt;/h3&gt;
&lt;p&gt;DQN uses another trick to stabilize the training process, which is to keep a memory of transitions instead of just using the most recent &lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt; transitions to update the value function. Can we use the same trick to stabilize A2C?&lt;/p&gt;

&lt;h3 id=&quot;entropy&quot;&gt;Entropy&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;&quot;&gt;Mnih et al. (2016)&lt;/a&gt; include the entropy of the policy as an additional term in their policy update. The idea is to &lt;em&gt;encourage&lt;/em&gt; policies to be more random than they would be otherwise, which can be seen as a way to encourage exploration (entropy is maximized by a uniform random variable–that is, a policy that picks actions at random with equal probability). The update now maximizes the quantity&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N \log \pi_{\theta}(a_i \vert s_i) A_i + \beta H(\pi_{\theta}(s_t)),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;H(\pi_{\theta}(s_t))&lt;/script&gt; is the entropy of the policy,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\pi_{\theta}(s_t)) = - \sum_{k=1}^K \pi_{\theta}(a_t = k \vert s_t) \log \pi_{\theta}(a_t = k \vert s_t)&lt;/script&gt;

&lt;p&gt;In Tensorflow, we adjust our policy update as follows:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;entropy_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# probabilities&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# log probabilities&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;policy_update&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;policy_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that there is no negative sign in front of the entropy calculation because we want to &lt;em&gt;maximize&lt;/em&gt; entropy, and therefore we need to minimize &lt;em&gt;negative&lt;/em&gt; entropy.&lt;/p&gt;</content><author><name>Colin Swaney</name></author><category term="research" /><category term="deep-learning" /><category term="reinforcement-learning" /><summary type="html">There are two standard algorithms in reinforcement learning: Q-learning and the policy gradient method. These methods represent orthogonal approaches. Policy gradient methods work on the policy function; Q-learning works on the action-value function. Policy gradient methods use Monte Carlo approximation; Q-learning uses bootstrap updates. And finally, policy gradient methods are low bias, high variance, while Q-learning is a low variance, high bias approach. In statistical learning, there is always some balance of bias and variance that provides the best outcome. Actor-critic methods provide, in some sense, a compromise between Q-learning and policy gradient methods, and can be thought of as a way to balance out the costs and benefits of these approaches.</summary></entry><entry><title type="html">Deep-Q Learning</title><link href="http://localhost:4000/research/development/2019/07/01/deep-Q-learning.html" rel="alternate" type="text/html" title="Deep-Q Learning" /><published>2019-07-01T00:00:00-04:00</published><updated>2019-07-01T00:00:00-04:00</updated><id>http://localhost:4000/research/development/2019/07/01/deep-Q-learning</id><content type="html" xml:base="http://localhost:4000/research/development/2019/07/01/deep-Q-learning.html">&lt;p&gt;An introduction to deep reinforcement learning using Deep-Q learning.&lt;/p&gt;</content><author><name>Colin Swaney</name></author><category term="development" /><category term="deep-learning" /><category term="reinforcement-learning" /><summary type="html">An introduction to deep reinforcement learning using Deep-Q learning.</summary></entry></feed>